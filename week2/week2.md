#### Before Week 2:
- Finish first 4 chapters of _Natural Language Processing with Python_  (if you haven't already)
- Watch **Edit Distance**:
    - [Defining Minimum Edit Distance](https://class.coursera.org/nlp/lecture/6)
    - [Computing Minimum Edit Distance](https://class.coursera.org/nlp/lecture/7)
    - [Backtrace for Computing Alignments](https://class.coursera.org/nlp/lecture/8)
    - [Weighted Minimum Edit Distance](https://class.coursera.org/nlp/lecture/9)
    - [Minimum Edit Distance in Computational Biology](https://class.coursera.org/nlp/lecture/10)

1. Monday 6/1: **Edit Distance**
    - Before Tuesday: Review API notes from 6002
2. Tuesday 6/2 – *Guest Instructor*: Jonathan Dinu – **Web Scraping**
    - Before Wednesday:
        - Watch **Information Retrieval**:
            - [Introduction to Information Retrieval](https://class.coursera.org/nlp/lecture/178)
            - [Term-Document Incidence Matrices](https://class.coursera.org/nlp/lecture/179)
            - [The Inverted Index](https://class.coursera.org/nlp/lecture/180)
            - [Query Processing with the Inverted Index](https://class.coursera.org/nlp/lecture/181)
            - [Phrase Queries and Positional Indexes](https://class.coursera.org/nlp/lecture/182)
        - Supplemental reading: 
            - [MR+S Chapter 1: Boolean Retrieval](http://nlp.stanford.edu/IR-book/pdf/01bool.pdf)
            - [MR+S Chapter 2: Term vocabulary and postings list, section 2.4](http://nlp.stanford.edu/IR-book/pdf/02voc.pdf)
3. Wednesday 6/3: **Information Retrieval**
    - Before Thursday:
        - Watch **Ranked Information Retrieval** 
            - [Introducing Ranked Retrieval](https://class.coursera.org/nlp/lecture/183)
            - [Scoring with the Jaccard Coefficient](https://class.coursera.org/nlp/lecture/184)
            - [Term Frequency Weighting](https://class.coursera.org/nlp/lecture/185)
            - [Inverse Document Frequency Weighting](https://class.coursera.org/nlp/lecture/186)
            - [TF-IDF Weighting](https://class.coursera.org/nlp/lecture/187)
            - [The Vector Space Model](https://class.coursera.org/nlp/lecture/188)
            - [Calculating TF-IDF Cosine Scores](https://class.coursera.org/nlp/lecture/189)
            - [Evaluating Search Engines](https://class.coursera.org/nlp/lecture/190)
        - Supplemental reading: 
            - [MR+S Chapter 6: Scoring, term weighting, and the vector space model](http://nlp.stanford.edu/IR-book/pdf/06vect.pdf)
            - [parts of Chapter 8 Evaluation in Information Retrieval](http://nlp.stanford.edu/IR-book/pdf/08eval.pdf)
4. Thursday 6/4: **Ranked Information Retrieval**

#### For Week 3:
- Before Monday:
    + Watch **Language Modeling**:
        * [Introduction to N-grams](https://class.coursera.org/nlp/lecture/14)
        * [Estimating N-gram Probabilities](https://class.coursera.org/nlp/lecture/128)
        * [Evaluation and Perplexity](https://class.coursera.org/nlp/lecture/129)
        * [Generalization and Zeros](https://class.coursera.org/nlp/lecture/17)
        * [Smoothing: Add-One](https://class.coursera.org/nlp/lecture/18)
        * [Interpolation](https://class.coursera.org/nlp/lecture/19)
        * [Good-Turing Smoothing](https://class.coursera.org/nlp/lecture/32)
        * [Kneser-Ney Smoothing](https://class.coursera.org/nlp/lecture/20)
- Before Thursday:
    + Read [chapter 5](http://www.nltk.org/book_1ed/ch05.html) of _Natural Language Processing with Python_